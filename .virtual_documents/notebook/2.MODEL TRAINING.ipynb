





# Basic Import 

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Modelling
from sklearn.metrics import precision_score,accuracy_score,recall_score,f1_score,roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.neighbors import KNeighborsClassifier

# Ignore warnings
import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)





df=pd.read_csv(r"C:\Users\HP\Desktop\projects\Adult_census_Income_Prediction\notebook\data\cleaned_Adult_dataset.csv")





df.head()


X=df.drop(['salary'],axis=1)
y=df['salary']


X


df['salary'].value_counts()


# Create Column Transformer with 3 types of transformers
num_features = X.select_dtypes(exclude="object").columns
cat_features = X.select_dtypes(include="object").columns

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

numeric_transformer = StandardScaler()
le_transformer = LabelEncoder()

X[num_features] = numeric_transformer.fit_transform(X[num_features])
for col in cat_features:
    X[col] = le_transformer.fit_transform(X[col])



print(num_features)
print(cat_features)


X.head()


y = y.map({' <=50K':0,' >50K':1})


X.shape,y.shape


pd.concat([X,y],axis=1).corr()





X = X.drop(["workclass","fnlwgt","education","occupation","race","country"],axis=1)
X


# separate dataset into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
X_train.shape, X_test.shape





def evaluate_classification_model(true, predicted):
    """
    Calculate classification evaluation metrics.

    Parameters:
        true (array-like): True class labels.
        predicted (array-like): Predicted class labels.

    Returns:
        accuracy (float): Classification accuracy.
        precision (float): Precision score.
        recall (float): Recall score.
        f1 (float): F1-score.
    """
    accuracy = accuracy_score(true, predicted)
    precision = precision_score(true, predicted)
    recall = recall_score(true, predicted)
    f1 = f1_score(true, predicted)
    return accuracy, precision, recall, f1



# Define class weights for balancing classes
class_priors = y_train.value_counts(normalize=True).to_dict()

# Define class weights for balancing classes
class_weights = {0: class_priors[0], 1: class_priors[1]}

classification_models = {
    "Logistic Regression": LogisticRegression(class_weight=class_weights),
    "Random Forest Classifier": RandomForestClassifier(class_weight=class_weights),
    "K-Neighbors Classifier": KNeighborsClassifier(),
    "Decision Tree Classifier": DecisionTreeClassifier(class_weight=class_weights),
    "XGBoost Classifier": XGBClassifier(),
    "CatBoost Classifier": CatBoostClassifier(verbose=0),
    "AdaBoost Classifier": AdaBoostClassifier(),
    "Naive Bayes": GaussianNB(priors=list(class_weights.values()))
}

model_list = []
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []

for model_name, model in classification_models.items():
    model.fit(X_train, y_train)  # Train model

    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Evaluate Train and Test dataset
    model_train_accuracy, model_train_precision, model_train_recall, model_train_f1 = evaluate_classification_model(y_train, y_train_pred)
    model_test_accuracy, model_test_precision, model_test_recall, model_test_f1 = evaluate_classification_model(y_test, y_test_pred)

    print(model_name)
    model_list.append(model_name)

    print('Model performance for Training set')
    print("- Accuracy Score: {:.4f}".format(model_train_accuracy))
    print("- Precision Score: {:.4f}".format(model_train_precision))
    print("- Recall Score: {:.4f}".format(model_train_recall))
    print("- F1 Score: {:.4f}".format(model_train_f1))

    print('----------------------------------')

    print('Model performance for Test set')
    print("- Accuracy Score: {:.4f}".format(model_test_accuracy))
    print("- Precision Score: {:.4f}".format(model_test_precision))
    print("- Recall Score: {:.4f}".format(model_test_recall))
    print("- F1 Score: {:.4f}".format(model_test_f1))

    accuracy_list.append(model_test_accuracy)
    precision_list.append(model_test_precision)
    recall_list.append(model_test_recall)
    f1_list.append(model_test_f1)

    print('=' * 35)
    print('\n')



pd.DataFrame(list(zip(model_list, accuracy_list, precision_list, recall_list, f1_list)), columns=["model_list", "accuracy_list", "precision_list", "recall_list", "f1_list"]).sort_values(by=["accuracy_list"],ascending=False)


            models = {
                "Decision Tree": DecisionTreeClassifier(),
                "Random Forest": RandomForestClassifier(),
                "Logistic Regression": LogisticRegression(),
                "K-Nearest Neighbors": KNeighborsClassifier(),
                "XGBoost": XGBClassifier(),
                "AdaBoost": AdaBoostClassifier(),
            }

            params = {
                "Decision Tree": {
                    'criterion': ['gini', 'entropy'],
                    'splitter': ['best', 'random'],
                },
                "Random Forest": {
                    'criterion': ['gini', 'entropy'],
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                },
                "Logistic Regression": {},
                "K-Nearest Neighbors": {
                    'n_neighbors': [3, 5, 7, 9],
                    'weights': ['uniform', 'distance'],
                },
                "XGBoost": {
                    'learning_rate': [0.01, 0.05, 0.1],
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                },
                "AdaBoost": {
                    'learning_rate': [0.01, 0.05, 0.1],
                    'n_estimators': [8, 16, 32, 64, 128, 256]
                }  

                
            }


from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report
def evaluate_models(X_train, y_train, X_test, y_test, models, param):

        report = {}

        for model_name, model in models.items():
            parameters = param.get(model_name, {})
            
            if parameters:
                grid_search = GridSearchCV(model, parameters, cv=3)
                grid_search.fit(X_train, y_train)
                best_model = grid_search.best_estimator_
            else:
                best_model = model

            best_model.fit(X_train, y_train)

            y_train_pred = best_model.predict(X_train)
            y_test_pred = best_model.predict(X_test)

            # Calculate classification metrics
            f1 = f1_score(y_test, y_test_pred, average='weighted')
            accuracy = accuracy_score(y_test, y_test_pred)
            precision = precision_score(y_test, y_test_pred, average='weighted')
            recall = recall_score(y_test, y_test_pred, average='weighted')
#            conf_matrix = confusion_matrix(y_test, y_test_pred)
#            class_report = classification_report(y_test, y_test_pred)

            report[model_name] = {
                "F1 Score": f1,
                "Accuracy": accuracy,
                "Precision": precision,
                "Recall": recall,
#                "Confusion Matrix": conf_matrix,
#                "Classification Report": class_report
            }

        return report



modelreport = evaluate_models(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, models=models, param=params)


pd.DataFrame.from_dict(modelreport,oreint="index")



